# Move37-Notes-Algorithms
This is the algorithms and notes from School of AI's Move37 course

### Lecture 1

We humans have been able to solve some hard problems in our relatively short existence on earth using our biological intelligence to analyze data and come up with solutions but some problems like major diseases extreme poverty long-term environmental sustainability have proven to be very hard to solve some of thes emajor problems may take large groups of well-trained humans decades to solve and we just don't have the luxury of waiting that long we need those solutions now since its inception in the 50s artificial intelligence researchers have always been driven by a mixture of curiosity of how the universe really works and altruism to help augment our human intelligence with machine learning systems and it turns out that AI isn't just a luxury it's a necessity the amount of data generated on the internet doubles every two years it's an exponential growth and the keys to so many mysteries like the ending of The Sopranos lie hidden in that data data that no single human could parse through alone but perhaps with the right algorithm could so if you've just started to learn how machine learning works you'll notice that the vast majority of the introductory algorithms and theories that you'll encounter in blog posts and courses will mostly all be encompassed under the category of supervised learning in the machine learning pipeline the first step is to collect a data set they come in all sorts of file formats and generally look like an Excel table of values each data point has a different value for whatever feature it has in supervised learning we're trying to predict value that already exists in our data this is called the label sometimes it's called the target variable they can also be called the dependent variable whereas the rest of the features are considered
independent variables so given our features like the years of experience the age and the educational background of an individual it would be helpful to be able to predict their salary or giventhe age of a car and the drivers age it would be helpful to be able to predict the risk of automotive accident the great thing here is that the data we'd use to train our model whichever one we pick already contains the desired
response that is it contains a dependent variable it's like having training wheels on a bike we could choose a
linear regression model logistic regression a neural network decision trees all have different ways of
eventually learning the function that relates the input features to the label most data however doesn't have clean labels for us to use but we still want to derive some insights from it that would be the field known as unsupervised learning we could then cluster our data using techniques like k-means and mixture models allowing us to visualize groups of data points that are related that we wouldn't know of otherwise we
can also attempt to find a compressed representation of the data using say an auto encoder then use that
representation for a specific task or we could just try to find the anomaly in the data set meaning identify what data point doesn't fit with the rest like say a fraudulent transaction unsupervised learning algorithms are generally used to pre-process data during say the exploratory analysis phase or for supervised learning algorithms supervised and unsupervised learning algorithms are supremely useful they are tools to recognize patterns in complex data but consider this scenario we're a new online product delivery startup and we've just deployed a fleet of vehicles and factories that help us move a product from point A to point B so many things could go wrong before the product is successfully delivered the trucks could breakdown bad weather could cause road closures the food could go bad what kind of learning technique should we use to predict the most optimal delivery route given all the other factors this is a highly dynamic problem space we need a learning system here that will be highly adaptive to changes and unfortunately we don't have a pre-existing data set to learn from we have to learn in real time what works and what doesn't work in a setting that introduces an entirely new dimension time this is the task that reinforcement learning attempts to solve it's somewhere in between supervised and unsupervised learning while in supervised learning we have a target label for each training example and in an unsupervised learning we don't have a label at all in reinforcement learning we have time delayed labels that are sparse meaning we don't get that many and based on this signal of time delay the labels which we can call rewards we can learn how to behave in this environment it's this powerful combination of pattern recognition networks and real-time environment based learning frameworks called deep reinforcement learning that has resulted in some incredible recent AI success stories including deep mines alphago program and open a high fives dota victory we'll discuss the details of those near the end of the course but first we need to understand the algorithms and theory of pure reinforcement learning all of it starts with defining some kind of mathematical framework that encapsulates the idea of an AI interacting with an environment or time as a dimension and it's learning through trial and error in 1906 a russian mathematician named Andre Markov was interested in modeling systems that followed a chain of linked events blockchain no so he defined what's now called a Markov chain to describe this process a Markov chain has a set of states in a process that can move successively from one state to another each move is a single step and is based on a transition Model T that defines how to move from one state to the next this chain is based on a property he also invented called the Markov property it states that given the present the future is conditionally independent of the past meaning the state in which the process is now is dependent only on the state it was at one time step ago for example let's say we want to use a Markov chain to model the weather with just two states sunny and cloudy using what's called a transition matrix we can represent the weather model in which a sunny day is 90% likely to be followed by another sunny day and a rainy day is 50% likely to be followed by another rainy day each states of the chain is a node in the chain graph and the transition probabilities are edges with the highest probabilities having the thickest edges the most common framework for representing the reinforcement learning problem of an agent learning in an environment is called a Markov decision process this is an extension of Markov chains the difference is the addition of actions meaning allowing choice and rewards giving motivation every Markov decision process is defined by five components a set of possible States an initial state a set of possible actions a transition model and a reward function the transition model returns the probability of reaching the next state if the action is done in a previous
state but given SN a the model is conditionally independent of all previous states and actions which is the Markov property and the reward function R returns a real value every time the agent moves from one state to the other and since we have a reward function we can conclude that some states are more desirable than others because when the agent moves to these states it receives a high reward the opposite is also true there are states that should be avoided because when the agent moves there it receives a negative reward the problem then is that agent has to maximize the reward by avoiding states which return negative values and choosing the one which returns positive values the solution is to find a policy which selects the action with the highest reward agents can try different policies but only one can be considered an optimal policy which gives us the best utility so if we for example have a delivery drone we want to get to our friend in the same room using the optimal path we can use the Markov decision process to frame this problem we can define our
environment as a matrix with the starting States being in one corner there could be obstacles in our
environment like ceiling lights and post-grad students that we want to avoid and our drone can go up down left or right we've got a set of states actions and rewards here and each environment will have its own characteristics that make it unique for example this particular environment is fully observable since our drone always knows which state it is in we can also say that there's no limit on time to deliver so the problem has an infinite horizon these factors will influence the choice of algorithm we use to find the best
policy the question then becomes how does an agent choose the best policy that's a topic for the next video three things to remember from this video though in reinforcement learning and AI learns how to optimally interact in a real-time environment using time delay labels called rewards as a signal the

Markov decision process is a mathematical framework for defining the reinforcement learning problem using States actions and rewards and through interacting with the environment an AI will learn a policy which will return an action for a given state with the highest reward.
 
